{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bab8a9-501a-4bf5-ad38-aaca08ce3ffb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style='font-size:40px'> \n",
    "    Dynamic Programming\n",
    "</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            Entendemos como \"Programação Dinâmica\" o um paradigma matemático que busca solucionar problemas complexos quebrando-os em problemas menores.\n",
    "        </li>\n",
    "        <li>\n",
    "            A premissa da Programação Dinâmica é a ideia de que o conjunto das soluções ótimas dos sub-problemas nos levará à solução ótima do problema maior.\n",
    "        </li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de097fa-7c5f-447a-b22e-5dc70e63b59d",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $\\displaystyle V(s)=\\max_{a\\in A(s)}{\\sum_{s'\\in S}{P_{a}(s'|s)[r(s,a,s')+\\gamma V(s')]}}$\n",
    "        <figcaption style='font-size:15px'>\n",
    "            Repare que a Equação de Bellman se baseia na premissa da Programação Dinâmica.\n",
    "        </figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e5de5-ec44-4c4e-a94e-1f8f496e13d1",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> \n",
    "    <a href='https://gibberblot.github.io/rl-notes/single-agent/value-iteration.html'>\n",
    "        Value Iteration\n",
    "    </a>\n",
    "</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Value Iteration é uma maneira de resolvermos problemas de Programação Dinâmica. Nele, atualizamos o valor de um state iterativamente até que esse valor pare de se alterar significativamente, ou atingemos um número específico de iterações.\n",
    "        </li>\n",
    "        <li>\n",
    "            Uma vez medido o valor do state atual, o atualizamos e executamos a ação de maior retorno esperado.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d1845-c9dd-48a4-b714-7bc7b1a365c6",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            <img src='img/05_gridworld_iteration.png'>\n",
    "        <figcaption style='font-size:15px'>\n",
    "            Valores de state de um Gridworld após 100 iterações de Value Iteration. \n",
    "        </figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15659072-c149-4d84-a7d2-b3b42d547943",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> \n",
    "    <a href='https://gibberblot.github.io/rl-notes/single-agent/temporal-difference-learning.html'>\n",
    "        Temporal Difference Reinforcement Learning\n",
    "    </a>\n",
    "</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Introdução ao conceito de Temporal Difference RL, um método model-free utilizado em casos onde não contamos com as probabilidades de transição de estados.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cbd2fc-f5cf-40ac-b2cc-f4ed1ed8e8e1",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Model-Based x Model-Free</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Problemas Model-Based são situações em que nós sabemos de antemão $P_a(s'|s)$ e $r(s,a,s')$. Value Iteration seria um método para cases Model-Based.\n",
    "        </li>\n",
    "        <li>\n",
    "            * Em situações Model-Free, nós não temos em mãos as probabilidades de transição, nem os rewards. Nesse caso, o algoritmo deverá criar a sua própria policy com base em tentativa e erro.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3afb408-dd2a-4924-b95d-bcb305f840c2",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Monte-Carlo Method</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            O Método de Monte-Carlo consiste numa abordagem para resolvermos problemas Model-Free.\n",
    "        </li>\n",
    "        <li>\n",
    "            Nele, simulamos o agente numa quantidade $n$ de episódios. Ao final, designamos o valor dos states como o discounted reward médio observado.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4f061-671a-485b-b224-b996de30cc2f",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $\\displaystyle V(s)=\\frac{1}{N}\\sum_{i=1}^{N}{G_i}$\n",
    "        <figcaption style='font-size:15px'>\n",
    "            Valor do state $s$, usando o Método de Monte-Carlo.\n",
    "        </figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21cfa9-7239-4636-9172-a11d015f236f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Caveats</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            Note que a coleção de todos os discounted rewards para um state pode ter alta variância.\n",
    "        </li>\n",
    "        <li>\n",
    "            Além disso, o Monte-Carlo Method pode ser ineficiente para problemas com episódios longos demais, uma vez que atualizará os valores dos states apenas quando todos esses acabarem.\n",
    "        </li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a6b4f-14df-4c8e-9153-6c7e3e66a79b",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Temporal Difference Learning</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "             TD Learning oferece uma eficiência computacional maior ao MC. Isso porque atualizamos os valores dos states apenas tendo em vista o reward recebido no state subsequente.\n",
    "        </li>\n",
    "        <li>\n",
    "            Além disso, essa técnica fornece uma maneira de atualizarmos o nosso modelo mais granularizada do que MC. Ao invés de $V(s)$, passamos a atualizar os valores esperados $Q(s,a)$.\n",
    "        </li>\n",
    "        <li>\n",
    "            Para calibrar o impacto dos rewards mais recentes, utilizamos uma learning-rate $\\alpha$.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845df948-6be4-4d96-8bba-3a42fdb4488a",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $\\displaystyle Q(s,a)_{\\text{new}}=Q(s,a)+\\alpha[r+\\gamma V(s')-Q(s,a)]$\n",
    "        <figcaption style='font-size:15px'>\n",
    "            Atualização dos valores esperados, levando em conta o reward obtido no novo episódio.\n",
    "        </figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f4f87-e158-4173-9494-1736c3f1357b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Explicação da Fórmula</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            A intenção da equação é fazermos algo parecido com uma média exponencial.\n",
    "        </li>\n",
    "        <li>\n",
    "            A presença de $-Q(s,a)$ serve para que o prêmio estimado do episódio $n-1$ seja ponderado por $(1-\\alpha)$.  \n",
    "        </li>\n",
    "        <li>\n",
    "            Seja $Q(s,a)_{1}$ o prêmio estimado no primeiro episódio de nosso problema, seu peso no episódio $n$ será $(1-\\alpha)^{n-1}$. \n",
    "        </li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7248aae1-cb54-4a18-b03b-2e3973a097df",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $$\\begin{align*}%\\label{}\n",
    "            \\displaystyle \n",
    "            &Q_2=Q_1 + \\alpha(-Q_1)=Q_1(1-\\alpha)\\\\\n",
    "            &Q_3=Q_2(1-\\alpha)=Q_1(1-\\alpha)^{2}\\\\\n",
    "            &\\vdots\\\\\n",
    "            &Q_{n}=Q_{n-1}(1-\\alpha)=Q_1(1-\\alpha)^{n-1}\n",
    "        \\end{align*}\n",
    "        $$\n",
    "        <figcaption style='font-size:15px'>\n",
    "            Demonstração da perda exponencial de importância de $Q_{1}$ com o passar dos episódios.\n",
    "        </figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a85a6-fccd-4fdd-bf31-1f40d6016e25",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Q-Learning</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            Q-Learning consiste numa modalidade de TDRL em que consideramos $\\displaystyle V(s')=\\operatorname{argmax}_{a'}{Q(s',a')}$.\n",
    "        </li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f178a9ad-2ec9-4d06-9b43-0e00cb43665d",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $$\\begin{align*}%\\label{}\n",
    "            \\displaystyle \n",
    "            &\\delta\\leftarrow[r+\\gamma \\cdot\\max_{a'}{Q(s',a')}-Q(s,a)]\\\\\n",
    "            &Q(s,a)\\leftarrow Q(s,a)+\\alpha \\delta\n",
    "        \\end{align*}\n",
    "        $$\n",
    "        <figcaption style='font-size:15px'>\n",
    "            Definição de um Q-Learning. No seu caso, denominamos a parte a ser multiplicada por $\\alpha$ como $\\delta$.\n",
    "        </figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e51839-e193-4899-b823-c6ad8e0fb6bd",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            A política de um problema de Q-Learning consistirá em sempre escolher a ação de maior reward esperado.\n",
    "        </li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ff1512-e913-4c3b-9817-fcbbd27735b9",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $$\\begin{align*}%\\label{}\n",
    "            \\displaystyle \n",
    "            \\pi(s)=\\operatorname{argmax}_{a\\in A(s)}{Q(s,a)}\n",
    "        \\end{align*}\n",
    "        $$\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d19f869-02cb-4f2b-b97d-0f09b22ea09f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> SARSA</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            Repare que o Q-Learning tem uma abordagem gananciosa em sua tomada de decisão; ele sempre vai considerar as ações de reward esperado máximo na atualização de $Q(s,a)$.\n",
    "        </li>\n",
    "        <li>\n",
    "            Com isso, podemos considerá-lo um algoritmo de <i>exploitation</i> forte. \n",
    "        </li>\n",
    "        <li>\n",
    "            O SARSA propõe adotar uma estratégia mais equilibrada. Nele, usamos um algoritmo de Multi-Armed Bandit na hora de escolhermos a ação que gerará $Q(s',a')$.\n",
    "        </li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2099c3-2b6e-4e95-ba7b-f61ae690b937",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $$\\begin{align*}%\\label{}\n",
    "            \\displaystyle \n",
    "            &\\delta\\leftarrow[r+\\gamma \\cdot \\overbrace{Q(s',a')}^{\\text{multi-armed bandit}}-Q(s,a)]\\\\\n",
    "            &Q(s,a)\\leftarrow Q(s,a)+\\alpha \\delta\n",
    "        \\end{align*}\n",
    "        $$\n",
    "        <figcaption style='font-size:15px'>\n",
    "            Em SARSA, escolhemos a próxima ação via Multi-Armed Bandit com base nos rewards $Q(s',a')$.\n",
    "        </figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be238b-5172-4706-a357-ea87c1d5d14b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> On-Policy x Off-Policy</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            Em Reinforcement Learning, podemos categorizar modelos como On-Policy e Off-Policy.\n",
    "        </li>\n",
    "        <li>\n",
    "            Um algoritmo <i> on-policy</i> aprende sua policy por experiência, explorando o ambiente durante o treinamento (com SARSA, por exemplo).\n",
    "        </li>\n",
    "        <li>\n",
    "            Já um algoritmo <i> off-policy</i> usa a policy pronta de um outro agente para a escolha das decisões, tomando sempre a ação com maior valor retornado durante o treinamento desse.\n",
    "        </li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf4f5a0-471c-4296-a3c8-80fa6fa84323",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> \n",
    "    <a href='https://gibberblot.github.io/rl-notes/single-agent/n-step.html'>\n",
    "        N-Step Reinforcement Learning\n",
    "    </a>\n",
    "</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Até aqui, exploramos o MC Learning, que aguarda a execução de todos os episódios para atualizar os valores dos estados, e os algoritmos de TD Learning, que consideram apenas o reward mais imediato nessa operação.   \n",
    "        </li>\n",
    "        <li>\n",
    "            O n-step Reinforcement Learning procura ser o meio termo entre essas abordagens, considerando os rewards dos $n$ steps seguintes na atualização de $Q(s,a)$.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479d6973-b8d9-4200-867b-6e9c4961a9ec",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Equação do N-Step Learning</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Recordemos que $G$ consiste na soma ponderada dos rewards futuros ao time-step $t$. Originalmente, nós somávamos os rewards até que chegássemos a uma convergência no valor.\n",
    "        </li>\n",
    "        <li>\n",
    "            No contexto do N-Step Learning, nós consideramos apenas os rewards dos $n$ steps seguintes ao atual. \n",
    "        </li>\n",
    "        <li>\n",
    "            A escolha de quais ações tomar podem tanto se basear no maior valor esperado (n-step Q-Learning), quanto via algoritmo Multi-Armed Bandit (n-step SARSA).\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a42befa-13e6-43b7-b877-0f0a9a473a05",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $\\displaystyle G_{t}^{n}=\\sum_{i=t+1}^{\\min{(t+n,T)}}{\\gamma^{i-t-1}r_{i}}$\n",
    "        <figcaption style='font-size:15px'> Repare que nós truncamos a soma, caso o time-step terminal ocorra antes de $t+n$</figcation>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d455f-5722-40a0-884f-d860481ae09f",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Finalmente, usamos essa medida na atualização de $Q(s,a)$.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a4af4-0d7a-4a23-90c3-568d294bb917",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $\\displaystyle Q(s_t,a_t)=Q(s_t,a_t)+\\alpha[G_{t}^{n}-Q(s_t,a_t)]$\n",
    "        <figcaption style='font-size:15px'> Comparando com TD Learning, $G_{t}^{n}$ substitui $r+\\gamma V(s')$.</figcation>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d5d16c-bb3f-480f-b9d4-13c914933cfb",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> \n",
    "    <a href='https://gibberblot.github.io/rl-notes/single-agent/mcts.html'>\n",
    "        Monte-Carlo Tree Search (MCTS)\n",
    "    </a>\n",
    "</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Algoritmo interessante para situações em que o espaço de ação e states é muito vasto.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb79df50-24d6-47ab-8f13-021f6bb4af82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master d6f039f] Continuar de \"Algorithm\"\n",
      " 2 files changed, 14 insertions(+), 14 deletions(-)\n",
      "Enumerating objects: 7, done.\n",
      "Counting objects: 100% (7/7), done.\n",
      "Delta compression using up to 24 threads\n",
      "Compressing objects: 100% (4/4), done.\n",
      "Writing objects: 100% (4/4), 504 bytes | 504.00 KiB/s, done.\n",
      "Total 4 (delta 3), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
      "To https://github.com/felipesveiga/reinforcement-learning.git\n",
      "   ff6b2d2..d6f039f  master -> master\n"
     ]
    }
   ],
   "source": [
    "! git add .\n",
    "! git commit -am 'Continuar de \"Action selection\"'\n",
    "! git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e53662-4f6d-4498-a741-f0da1006e8b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<p style='color:red'> Iniciei Monte-Carlo Tree Search (MCTS); Continuar de \"Action selection\"; Explicar MCTS; Pesquisar sobre Online e Offline Planning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
