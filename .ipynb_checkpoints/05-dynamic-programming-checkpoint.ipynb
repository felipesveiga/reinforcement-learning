{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00bab8a9-501a-4bf5-ad38-aaca08ce3ffb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style='font-size:40px'> \n",
    "    Dynamic Programming\n",
    "</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            Entendemos como \"Programação Dinâmica\" o um paradigma matemático que busca solucionar problemas complexos quebrando-os em problemas menores.\n",
    "        </li>\n",
    "        <li>\n",
    "            A premissa da Programação Dinâmica é a ideia de que o conjunto das soluções ótimas dos sub-problemas nos levará à solução ótima do problema maior.\n",
    "        </li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de097fa-7c5f-447a-b22e-5dc70e63b59d",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $\\displaystyle V(s)=\\max_{a\\in A(s)}{\\sum_{s'\\in S}{P_{a}(s'|s)[r(s,a,s')+\\gamma V(s')]}}$\n",
    "        <figcaption style='font-size:15px'>\n",
    "            Repare que a Equação de Bellman se baseia na premissa da Programação Dinâmica.\n",
    "        </figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e5de5-ec44-4c4e-a94e-1f8f496e13d1",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> \n",
    "    <a href='https://gibberblot.github.io/rl-notes/single-agent/value-iteration.html'>\n",
    "        Value Iteration\n",
    "    </a>\n",
    "</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Value Iteration é uma maneira de resolvermos problemas de Programação Dinâmica. Nele, atualizamos o valor de um state iterativamente até que esse valor pare de se alterar significativamente, ou atingemos um número específico de iterações.\n",
    "        </li>\n",
    "        <li>\n",
    "            Uma vez medido o valor do state atual, o atualizamos e executamos a ação de maior retorno esperado.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d1845-c9dd-48a4-b714-7bc7b1a365c6",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            <img src='img/05_gridworld_iteration.png'>\n",
    "        <figcaption style='font-size:15px'>\n",
    "            Valores de state de um Gridworld após 100 iterações de Value Iteration. \n",
    "        </figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15659072-c149-4d84-a7d2-b3b42d547943",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> \n",
    "    <a href='https://gibberblot.github.io/rl-notes/single-agent/temporal-difference-learning.html'>\n",
    "        Temporal Difference Reinforcement Learning\n",
    "    </a>\n",
    "</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            \n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb79df50-24d6-47ab-8f13-021f6bb4af82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 98fa447] Ver Temporal difference reinforcement learning\n",
      " 3 files changed, 66 insertions(+), 28 deletions(-)\n",
      " create mode 100644 img/05_gridworld_iteration.png\n",
      "Enumerating objects: 10, done.\n",
      "Counting objects: 100% (10/10), done.\n",
      "Delta compression using up to 24 threads\n",
      "Compressing objects: 100% (6/6), done.\n",
      "Writing objects: 100% (6/6), 23.25 KiB | 23.25 MiB/s, done.\n",
      "Total 6 (delta 4), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
      "To https://github.com/felipesveiga/reinforcement-learning.git\n",
      "   568990c..98fa447  master -> master\n"
     ]
    }
   ],
   "source": [
    "! git add .\n",
    "! git commit -am 'Iniciar Temporal difference reinforcement learning, introduzindo o conceito de model-based e model-free'\n",
    "! git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e53662-4f6d-4498-a741-f0da1006e8b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<p style='color:red'> Iniciar Temporal difference reinforcement learning, introduzindo o conceito de model-based e model-free</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
