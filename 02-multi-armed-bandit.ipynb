{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b080581-43d1-4eee-afb5-d76feeec9878",
   "metadata": {},
   "source": [
    "<h1 style='font-size:40px'> Return of the Multi-Armed Bandit</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eb6a26-d222-4072-ae3b-1d76a63fc644",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> \n",
    "    Section Introduction: The Explore-Exploit Dilemma\n",
    "</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Em análises estatísticas, a amostra requerida para termos um grande poder estatístico pode ser alta demais. Nesse caso, os pesquisadores podem se sentir tentados a sacrificar parte desse poder com um grupo menor, a fim de colher mais rapidamente os resultados, podendo usufruir das conclusões tiradas.\n",
    "        </li>\n",
    "        <li>\n",
    "            Esse conflito de interesses é conhecido como <i> Explore-Exploit Dilemma</i>.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc5e11-8c66-4e87-a2b2-d7f35c129dc4",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Aplicações do Explore-Exploit Dilemma</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            O Explore-Exploit Dilemma pode ser bastante sentido em pesquisas de mercado. Quando uma empresa decide realizar dois tipos de comerciais, está pondo em risco a imagem de seu produto para o público exposto ao de menor popularidade. Ao mesmo tempo, a companhia deseja ter uma amostra grande o bastante para confiar nas conclusões do estudo.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b122c10-c6e1-4e12-8b13-d0f44b76a83e",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> \n",
    "    Epsilon-Greedy Theory\n",
    "</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Epsilon-Greedy é um dos métodos fundamentais de RL. Ele é baseado no algoritmo <i>Greedy</i>\n",
    "        </li>\n",
    "        <li>\n",
    "            Uma busca Greedy consistiria em apenas escolhermos a opção com maior probabilidade de sucesso.\n",
    "        </li>\n",
    "        <li>\n",
    "            Por exemplo, suponha que jogamos em dois caça-níqueis, vencendo no primeiro, e perdendo no segundo. Pela lógica Greedy, deveremos jogar apenas no primeiro caça-níquel infinitamente.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc1fcb7-c658-4c7d-bd4b-753dd3c4d73b",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Lógica do Epsilon-Greedy</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Mas é claro: o algoritmo Greedy não leva em conta o tamanho de nossa amostragem. Por isso, o Epsilon-Greedy impõe uma pequena probabilidade $\\epsilon$ de executarmos uma opção aleatória à cada iteração.\n",
    "        </li>\n",
    "        <li>\n",
    "            Dessa forma, nos permitimos explorar a rota de maior sucesso, ao mesmo tempo em que construímos uma amostragem grande o bastante para avaliarmos todas as demais. \n",
    "        </li>\n",
    "        <li>\n",
    "            Costumamos definir $\\epsilon$ como 5% ou 10%. Ainda é possível configurarmos um decaimento, no decorrer das iterações.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a83f8ad-f49b-44a5-b0ca-fcbc4b8f8a27",
   "metadata": {},
   "source": [
    "<figure>\n",
    "        <center style='margin-top:20px'>\n",
    "            <img src='img/02_epsilon_decay.png'>\n",
    "                <figcaption> Opções de Epsilon Decay.</figcaption>\n",
    "        </center>\n",
    "    </figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40a9d773-1c4e-401e-858d-c0a896d86548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 5f8248b] Usar Aula 7 para escrever a seção \"Aplicações do Explore-Exploit Dilemma\"\n",
      " 2 files changed, 84 insertions(+), 24 deletions(-)\n",
      "Enumerating objects: 7, done.\n",
      "Counting objects: 100% (7/7), done.\n",
      "Delta compression using up to 24 threads\n",
      "Compressing objects: 100% (4/4), done.\n",
      "Writing objects: 100% (4/4), 1.16 KiB | 1.16 MiB/s, done.\n",
      "Total 4 (delta 3), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
      "To https://github.com/felipesveiga/reinforcement-learning.git\n",
      "   37c89d2..5f8248b  master -> master\n"
     ]
    }
   ],
   "source": [
    "! git add .\n",
    "! git commit -m 'Aula 9'\n",
    "! git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce606c-4113-40fa-ad46-6eb2b87c2172",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<p style='color:red'> Vi Aula 8; Aula 9</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
