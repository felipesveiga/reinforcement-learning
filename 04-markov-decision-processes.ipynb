{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf259ba-9cba-4802-8b55-1c232e437177",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style='font-size:40px'> \n",
    "    Markov Decision Processes\n",
    "</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            Seção em que vamos estudar os Markov Decision Processes, algoritmo cujas previsões se baseiam na sequência de estados apresentada.\n",
    "        </li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09380842-43af-43c1-8564-2c37c13ceb82",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Gridworld\n",
    "</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            Todo algoritmo de RL deve ser exposto a um ambiente, dentro do qual aprenderá ações que otimizem uma função loss condizente ao nosso objetivo. \n",
    "        </li>\n",
    "        <li>\n",
    "            No nosso caso, lidaremos com o Gridworld. Esse consiste numa matriz $3\\times{4}$. Um agente, inicializado em $(3,1)$, deverá caminhar até $(1,4)$, onde receberá um reward. No entanto, ele pode ser punido, caso passe por posições proibidas. \n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e79a2-39e6-4fb3-af38-d87c8c1b8cd8",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            <img src='img/04_gridworld.png'>\n",
    "        <figcaption style='font-size:15px'>\n",
    "            Representação do Gridworld.   \n",
    "        </figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb79df50-24d6-47ab-8f13-021f6bb4af82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 989f597] Iniciar seção 4\n",
      " 4 files changed, 236 insertions(+), 84 deletions(-)\n",
      " create mode 100644 .ipynb_checkpoints/04-markov-decision-processes-checkpoint.ipynb\n",
      " create mode 100644 04-markov-decision-processes.ipynb\n",
      "Enumerating objects: 8, done.\n",
      "Counting objects: 100% (8/8), done.\n",
      "Delta compression using up to 24 threads\n",
      "Compressing objects: 100% (5/5), done.\n",
      "Writing objects: 100% (5/5), 1.41 KiB | 1.41 MiB/s, done.\n",
      "Total 5 (delta 4), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (4/4), completed with 3 local objects.\u001b[K\n",
      "To https://github.com/felipesveiga/reinforcement-learning.git\n",
      "   db5acfc..989f597  master -> master\n"
     ]
    }
   ],
   "source": [
    "! git add .\n",
    "! git commit -m 'Definir \"episode\",\"terminal state\" e \"state space\"'\n",
    "! git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0471fe3-af30-45be-83fa-d211573e1457",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<p style='color:red'> Vi Aula 34; Iniciei Aula 35; Definir \"episode\",\"terminal state\" e \"state space\" ;Aula 35 (7:00)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d1e78-1ca9-4b3a-b590-27b3f8906927",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<p style='color:red'> Esperar até o final da seção 4 para começar a programar o framework de RL, por conta da Aula 30</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
