{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf259ba-9cba-4802-8b55-1c232e437177",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style='font-size:40px'> \n",
    "    Markov Decision Processes\n",
    "</h1>\n",
    "<div> \n",
    "    <ul style='font-size:20px'>\n",
    "        <li>\n",
    "            Seção em que vamos estudar os Markov Decision Processes, algoritmo cujas previsões se baseiam na sequência de estados apresentada.\n",
    "        </li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09380842-43af-43c1-8564-2c37c13ceb82",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Gridworld\n",
    "</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            Todo algoritmo de RL deve ser exposto a um ambiente, dentro do qual aprenderá ações que otimizem uma função loss condizente ao nosso objetivo. \n",
    "        </li>\n",
    "        <li>\n",
    "            No nosso caso, lidaremos com o Gridworld. Esse consiste numa matriz $3\\times{4}$. Um agente, inicializado em $(3,1)$, deverá caminhar até $(1,4)$, onde receberá um reward. No entanto, ele pode ser punido, caso passe por posições proibidas. \n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e79a2-39e6-4fb3-af38-d87c8c1b8cd8",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            <img src='img/04_gridworld.png'>\n",
    "        <figcaption style='font-size:15px'>\n",
    "            Representação do Gridworld.   \n",
    "        </figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436686e0-ef79-4eb5-8510-ef81997d1324",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Terminologias</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            <i>Episode</i>: Chamamos de \"episódio\" uma determinada simulação durante o treinamento.\n",
    "        </li>\n",
    "        <li> \n",
    "            <i> State Space</i>: Conjunto de todos os estados possíveis do nosso experimento.\n",
    "        </li>\n",
    "        <li> \n",
    "            <i> Terminal State</i>: Estado no qual um episódio se encerra.\n",
    "        </li>\n",
    "        <li>\n",
    "            <i> Policy</i>: Função cujo resultado discerne bons dos maus resultados.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7041c24-a0c0-47f5-874b-86c2be8026a4",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> Choosing Rewards\n",
    "</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>  \n",
    "            Ao concebermos um algoritmo de Aprendizado de Reforço, devemos fazer com que a sua policy seja condizente com os objetivos almejados.\n",
    "        </li>\n",
    "        <li>\n",
    "            Por exemplo, num algoritmo de GPS, poderíamos penalizar o modelo por cada minuto levado para o alcance do destino. Com isso, o orientamos a sempre buscar os caminhos mais rápidos. \n",
    "        </li>\n",
    "        <li>\n",
    "            Dependendo do quão flexível a policy for, o algoritmo poderá encontrar soluções inusitadas para o problema, como sugere esse <a href='https://www.youtube.com/watch?v=n0Cpqzqzroo'>vídeo</a>.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d7726-bf52-47fb-ae77-24c34015aa44",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> The Markov Property\n",
    "</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Aula em que é apresentada a Propriedade de Markov, que baseia os Markov Decision Processes.\n",
    "        </li>\n",
    "        <li>\n",
    "            Ela afirma que a probabilidade de um estado $s_{t}$ é apenas condicionada pelo estado diretamente anterior $s_{t-1}$.\n",
    "        </li>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a5815-0e08-4025-ab15-b952fa67061d",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $\\displaystyle P(s_{t}|s_{t-1},\\ldots,s_{1})=P(s_{t}|s_{t-1})$\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74da5e-cea4-425d-9468-7867c20f103e",
   "metadata": {},
   "source": [
    "<h2 style='font-size:30px'> \n",
    "    <a href='https://builtin.com/machine-learning/markov-decision-process'>\n",
    "        Markov Decision Processes (MDPs)\n",
    "    </a>\n",
    "</h2>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Markov Decision Processes é um algoritmo baseado na Propriedade de Markov. Ele assume estar em um ambiente em que ações não têm consequências determinísticas.\n",
    "        </li>\n",
    "        <li>\n",
    "            O seu objetivo é formular uma policy $\\pi$ que otimize a premiação do algoritmo a longo prazo.\n",
    "        </li>\n",
    "            Os principais componentes de um MDP são:\n",
    "            <ul>\n",
    "                <li>\n",
    "                    O espaço de states $S$.\n",
    "                </li>\n",
    "                <li>\n",
    "                    O state inicial $s_{0}\\in{S}$.\n",
    "                </li>\n",
    "                <li>\n",
    "                    O conjunto de ações possíveis num determinado state ($A(S)\\subseteq{A}$).\n",
    "                </li>\n",
    "                <li>\n",
    "                    As probabilidades de transição $P_{a}(s'|s),s\\in{S}, a\\in{A(s)}$. Elas mencionam a probabilidade de irmos para um state $s'$, executando a ação $a$ em $s$.        \n",
    "                </li>\n",
    "                <li>\n",
    "                    Os Rewards dados ao agente pela transição $s\\to{s'}$, por meio da ação $a$. \n",
    "                </li>\n",
    "                <li>\n",
    "                    Um Discount Factor $\\gamma \\in{[0,1]}$.\n",
    "                </li>\n",
    "            </ul>\n",
    "    </ul>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c30c80e-eabd-4b30-b213-916a97f30643",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            <img src='img/04_mdp.png'>\n",
    "        <figcaption style='font-size:15px'>\n",
    "            Ilustração de um Markov Decision Processes.   \n",
    "        </figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f861c5-b7dc-4b00-8899-a2d8f7564529",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Return ($G_{t}$)</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            $G_{t}$ consiste na soma de todos os rewards que serão recebidos do time-step $t$ em diante.\n",
    "        </li>\n",
    "        <li>\n",
    "            Como nosso ambiente não é determinístico, não faz sentido darmos o mesmo peso para todos os rewards. Quanto mais afastados eles estiverem de $t$, menos prováveis eles serão.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcd5dfd-aa69-4088-8092-d32e2db6d076",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $\\displaystyle G_{t}=\\sum_{k=0}^{\\infty}{\\gamma^{k}R_{t+k+1}}$\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b9a3ce-576f-49d8-a5c7-ec2d43b512c6",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Policy ($\\pi$)</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            A policy nada mais é do que uma distribuição probabilística de todas as ações possíveis, dado o state atual.\n",
    "        </li>\n",
    "        <li>\n",
    "            Quando criamos um agente, podemos determinar que a policy sempre escolha a ação de maior probabilidade. Outra abordagem seria uma decisão semi-aleatorizada ponderada pelas probabilidades de cada ação.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113f0d04-57c6-406b-b457-f4469f45156d",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $\\displaystyle \\pi(a|s)=P\\left[A_{t}=a|S_{t}=s\\right]$\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c561288-dde5-49f3-98b6-a0bc96226c88",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> Value Functions</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li>\n",
    "            Value functions são funções que buscam mensurar o reward a longo prazo de um state ou ação.\n",
    "        </li>\n",
    "        <li>\n",
    "            No caso de MDP's, usamos a Bellman Equation como Value Function. Ela recorre a $G_{t}$ para computar esse valor futuro.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6031a4-bca7-4948-81ed-28f0e334a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link muito bom também para Bellman Equation: https://gibberblot.github.io/rl-notes/single-agent/MDPs.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb383e5-222d-4f85-9b5c-45502654b07f",
   "metadata": {},
   "source": [
    "<h3 style='font-size:30px;font-style:italic'> \n",
    "    <a href='https://neptune.ai/blog/markov-decision-process-in-reinforcement-learning'>Bellman Equation</a>\n",
    "</h3>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A equação de Bellman fornece um meio de computarmos o valor de um state $s$, considerando a ação $a$ que fornece o maior reward esperado.\n",
    "        </li>\n",
    "        <li>\n",
    "            Vale ressaltar que a equação considera que as ações não são determinísticas, podendo levar o agente a mais de um state $s'$ diferente.\n",
    "        </li>\n",
    "        <li>\n",
    "            Com ela, mensuramos o reward esperado de cada ação possível em $s$, escolhendo aquela de maior valor:\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22887b3-1ed5-4ac1-b493-03d93a45cdcb",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $\\displaystyle V(s)=\\max_{a\\in A(s)}{\\sum_{s'\\in S}{P_{a}(s'|s)[r(s,a,s')+\\gamma V(s')]}}$\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f43725b-8842-4da9-9fbf-2e422ac89339",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Q-Values</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Denominamos como Q-Values os valores esperados de cada ação possível num state $s$. Basicamente, eles são os componentes dos vetores de onde queremos tirar o $\\max$.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f21047-7f70-478b-b6ae-ecf1c8386d16",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $\\displaystyle Q(s,a)=\\sum_{s'\\in S}{P_{a}(s'|s)[r(s,a,s')+\\gamma V(s')]}$\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe639f12-895a-4aad-a2f7-8fe02a902b2d",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "           Podemos assim escrever a Bellman Equation como o valor do maior Q-Value de um state $s$.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798cdb77-e718-42f5-8702-1470c87be78b",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $\\displaystyle V(s)=\\max_{a \\in A(s)}{Q(s,a)}$\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21758624-fdcd-4b3f-91b4-ec66d7f457ef",
   "metadata": {},
   "source": [
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            A ação a ser selecionada pela policy do modelo poderá ser representada como:\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552ecb9-e5f9-4ab8-87e5-cce351a7c03a",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $\\displaystyle \\pi(s)=\\operatorname{argmax}_{a \\in A(s)}{Q(s,a)}$\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a726f4c-537e-48d7-84be-157d52003b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição Alternativa Builtin:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817427f0-6ed0-4a14-bcb1-9be9515cb53a",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $$\\begin{align*}%\\label{}\n",
    "        \\displaystyle v(s)&=E[G_{t}|S_{t}=s]\\\\\n",
    "        &=E\\left[\\sum_{k=0}^{\\infty}{\\gamma^{k}R_{t+k+1}}|S_{t}=s\\right]\\\\\n",
    "        &=E\\left[R_{t+1}+\\gamma{G_{t+1}}|S_{t}=s\\right]\\\\\n",
    "        &=E\\left[R_{t+1}+\\gamma{v(S_{t+1})}\\right|S_{t}=s]\n",
    "        \\end{align*}\n",
    "        $$\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a86b5-3fba-4e19-876e-9063d1a424ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4 style='font-size:30px;font-style:italic;text-decoration:underline'> Por que $E\\left[R_{t+1}+\\gamma{G_{t+1}}|S_{t}=s\\right]=E\\left[R_{t+1}+\\gamma{v(S_{t+1})}\\right|S_{t}=s]$?</h4>\n",
    "<div> \n",
    "    <ul style='font-size:20px'> \n",
    "        <li> \n",
    "            Seja $E[v(S_{t+1})|S_{t}=s]=E[E[G_{t+1}|S_{t+1}]|S_{t}=s]$, podemos afirmar que $E[E[G_{t+1}|S_{t+1}]|S_{t}=s]=E[G_{t+1}|S_{t}=s]$ pela Law of Iterated Expectations.\n",
    "        </li>\n",
    "        <li>\n",
    "            Portanto, $E[v(S_{t+1})|S_{t}=s]=E[G_{t+1}|S_{t}=s]$.\n",
    "        </li>\n",
    "        <li>\n",
    "            Agora, voltando à equação original: \n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a242b8-a99f-44b5-8d82-f97b05df1456",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <center style='font-size:20px;margin-top:20px'> \n",
    "            $$\\begin{align*}%\\label{}\n",
    "        \\displaystyle v(s)&=E[R_{t+1}+\\gamma{G_{t+1}}|S_{t}=s]\\\\\n",
    "                          &=E[R_{t+1}|S_{t}=s]+E[\\gamma G_{t+1}|S_{t}=s]\\text{ (Valor Esperado é distributivo)}\\\\\n",
    "                          &=E[R_{t+1}|S_{t}=s]+\\gamma E[G_{t+1}|S_{t}=s]\\\\\n",
    "                          &=E[R_{t+1}|S_{t}=s]+\\gamma E[v(S_{t+1})|S_{t}=s]\\text{ (Como demonstrado acima)}\\\\\n",
    "                          &=E[R_{t+1}+\\gamma v(S_{t+1})|S_{t}=s]\n",
    "        \\end{align*}\n",
    "        $$\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a49551f-8a5f-4a4d-ba38-0cea314e0f69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<function __main__.best()>: {'n': 947, 'successes': 754},\n",
       " <function __main__.worst()>: {'n': 55, 'successes': 6}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl.stateless import StatelessAgent\n",
    "from rl.stateless.optimizers.epsilon_greedy import EpsilonGreedy\n",
    "import numpy as np\n",
    "\n",
    "def best():\n",
    "    if np.random.random()<.8:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def worst():\n",
    "    if np.random.random()<.1:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "s = StatelessAgent(routes=[best, worst], optimizer=EpsilonGreedy(.05), checkpoint_config={'n':10, 'output_path':'data/'})\n",
    "\n",
    "for _ in range(1000):\n",
    "    s.evaluate()\n",
    "s.routes_stats_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74d7d443-9337-4166-8270-6f0c2d04b910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<function __main__.best()>: {'n': 2, 'successes': 2},\n",
       " <function __main__.worst()>: {'n': 1, 'successes': 0}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.routes_stats_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb79df50-24d6-47ab-8f13-021f6bb4af82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 4350c27] Revisar Epsion-Greedy\n",
      " 11 files changed, 111 insertions(+), 73 deletions(-)\n",
      " create mode 100644 rl/stateless/__pycache__/__init__.cpython-310.pyc\n",
      " create mode 100644 rl/stateless/__pycache__/types.cpython-310.pyc\n",
      " create mode 100644 rl/stateless/__pycache__/utils.cpython-310.pyc\n",
      " create mode 100644 rl/stateless/optimizers/__pycache__/base.cpython-310.pyc\n",
      " create mode 100644 rl/stateless/optimizers/__pycache__/epsilon_greedy.cpython-310.pyc\n",
      "Enumerating objects: 28, done.\n",
      "Counting objects: 100% (28/28), done.\n",
      "Delta compression using up to 24 threads\n",
      "Compressing objects: 100% (18/18), done.\n",
      "Writing objects: 100% (18/18), 7.09 KiB | 7.09 MiB/s, done.\n",
      "Total 18 (delta 7), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (7/7), completed with 7 local objects.\u001b[K\n",
      "To https://github.com/felipesveiga/reinforcement-learning.git\n",
      "   f4831d5..4350c27  master -> master\n"
     ]
    }
   ],
   "source": [
    "! git add .\n",
    "! git commit -m 'Documentar código escrito até agora'\n",
    "! git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0471fe3-af30-45be-83fa-d211573e1457",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<p style='color:red'> Revisei Epsilon-Greedy; Documentar código escrito até agora; Programar Hoeffding, Bayesian Bandit</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d1e78-1ca9-4b3a-b590-27b3f8906927",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<p style='color:red'> Esperar até o final da seção 4 para começar a programar o framework de RL, por conta da Aula 30</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
